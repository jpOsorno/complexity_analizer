"""
Comparador LLM - Valida anÃ¡lisis de complejidad con IA
======================================================

Compara nuestro anÃ¡lisis estÃ¡tico con el anÃ¡lisis del LLM.
"""

import sys
import os
from typing import Dict, Optional, Tuple
from dataclasses import dataclass, field

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from llm_config import LLMConfig, ComplexityPrompts
from llm_client import GroqClient, LLMResponse


@dataclass
class ComparisonResult:
    """Resultado de comparar nuestro anÃ¡lisis con el del LLM"""
    
    # AnÃ¡lisis nuestro
    our_worst: str
    our_best: str
    our_average: str
    
    # AnÃ¡lisis del LLM
    llm_worst: str
    llm_best: str
    llm_average: str
    
    # ComparaciÃ³n
    agrees: bool
    confidence: float
    differences: str = ""
    llm_explanation: str = ""
    
    # Metadata
    latency_ms: float = 0.0
    llm_raw_response: str = ""
    
    def __str__(self):
        status = "âœ… COINCIDE" if self.agrees else "âš ï¸  DIFIERE"
        
        result = f"""
{'='*70}
COMPARACIÃ“N: {status}
{'='*70}

NUESTRO ANÃLISIS:
  Peor caso:     {self.our_worst}
  Mejor caso:    {self.our_best}
  Caso promedio: {self.our_average}

LLM ANÃLISIS (Llama 3.3 70B):
  Peor caso:     {self.llm_worst}
  Mejor caso:    {self.llm_best}
  Caso promedio: {self.llm_average}

EXPLICACIÃ“N DEL LLM:
{self.llm_explanation}
"""
        
        if not self.agrees and self.differences:
            result += f"""
DIFERENCIAS DETECTADAS:
{self.differences}
"""
        
        result += f"""
Confianza del LLM: {self.confidence*100:.0f}%
Latencia: {self.latency_ms:.0f}ms
{'='*70}
"""
        return result


class LLMComparator:
    """Compara anÃ¡lisis de complejidad con LLM"""
    
    def __init__(self, api_key: Optional[str] = None):
        """
        Inicializa el comparador.
        
        Args:
            api_key: API key de Groq (si None, busca en env)
        """
        if api_key is None:
            config = LLMConfig.from_env()
            api_key = config.api_key
        
        self.client = GroqClient(api_key)
        self.prompts = ComplexityPrompts()
    
    # ========================================================================
    # COMPARACIÃ“N: ALGORITMOS ITERATIVOS
    # ========================================================================
    
    def compare_iterative(
        self, 
        code: str,
        our_worst: str,
        our_best: str,
        our_average: str,
        explanation: str = ""
    ) -> ComparisonResult:
        """
        Compara anÃ¡lisis iterativo con LLM.
        
        Args:
            code: CÃ³digo pseudocÃ³digo
            our_worst: Nuestro anÃ¡lisis de peor caso
            our_best: Nuestro anÃ¡lisis de mejor caso
            our_average: Nuestro anÃ¡lisis de caso promedio
            explanation: Nuestra explicaciÃ³n (opcional)
            
        Returns:
            ComparisonResult con la comparaciÃ³n
        """
        # Construir descripciÃ³n de nuestro anÃ¡lisis
        our_analysis = f"""
Peor caso: {our_worst}
Mejor caso: {our_best}
Caso promedio: {our_average}
{f'ExplicaciÃ³n: {explanation}' if explanation else ''}
        """.strip()
        
        # Generar prompt
        prompt = self.prompts.iterative_analysis(code, our_analysis)
        
        # Consultar LLM
        response = self.client.analyze(prompt)
        
        # Procesar respuesta
        if not response.success or not response.parsed_json:
            # Si falla, asumir que coincide (sin validaciÃ³n)
            return ComparisonResult(
                our_worst=our_worst,
                our_best=our_best,
                our_average=our_average,
                llm_worst="N/A",
                llm_best="N/A",
                llm_average="N/A",
                agrees=True,  # Asumimos correcto si no hay validaciÃ³n
                confidence=0.0,
                differences=f"No se pudo validar con LLM: {response.error}",
                latency_ms=response.latency_ms,
                llm_raw_response=response.raw_text
            )
        
        # Extraer anÃ¡lisis del LLM
        llm_data = response.parsed_json
        
        llm_worst = llm_data.get("worst_case", "N/A")
        llm_best = llm_data.get("best_case", "N/A")
        llm_average = llm_data.get("average_case", "N/A")
        
        agrees = llm_data.get("agrees_with_our_analysis", True)
        confidence = llm_data.get("confidence", 0.5)
        differences = llm_data.get("differences", "")
        llm_explanation = llm_data.get("explanation", "")
        
        return ComparisonResult(
            our_worst=our_worst,
            our_best=our_best,
            our_average=our_average,
            llm_worst=llm_worst,
            llm_best=llm_best,
            llm_average=llm_average,
            agrees=agrees,
            confidence=confidence,
            differences=differences,
            llm_explanation=llm_explanation,
            latency_ms=response.latency_ms,
            llm_raw_response=response.raw_text
        )
    
    # ========================================================================
    # COMPARACIÃ“N: ALGORITMOS RECURSIVOS
    # ========================================================================
    
    def compare_recursive(
        self,
        code: str,
        our_equation: str,
        our_worst: str,
        our_best: str,
        our_average: str,
        solution_explanation: str = ""
    ) -> ComparisonResult:
        """
        Compara anÃ¡lisis recursivo con LLM.
        
        Args:
            code: CÃ³digo pseudocÃ³digo
            our_equation: Nuestra ecuaciÃ³n de recurrencia
            our_worst/best/average: Nuestras soluciones
            solution_explanation: ExplicaciÃ³n de nuestra soluciÃ³n
            
        Returns:
            ComparisonResult con la comparaciÃ³n
        """
        # Construir descripciÃ³n de nuestra soluciÃ³n
        our_solution = f"""
Peor caso: {our_worst}
Mejor caso: {our_best}
Caso promedio: {our_average}
{f'ExplicaciÃ³n: {solution_explanation}' if solution_explanation else ''}
        """.strip()
        
        # Generar prompt
        prompt = self.prompts.recursive_analysis(code, our_equation, our_solution)
        
        # Consultar LLM
        response = self.client.analyze(prompt)
        
        # Procesar respuesta
        if not response.success or not response.parsed_json:
            return ComparisonResult(
                our_worst=our_worst,
                our_best=our_best,
                our_average=our_average,
                llm_worst="N/A",
                llm_best="N/A",
                llm_average="N/A",
                agrees=True,
                confidence=0.0,
                differences=f"No se pudo validar con LLM: {response.error}",
                latency_ms=response.latency_ms,
                llm_raw_response=response.raw_text
            )
        
        llm_data = response.parsed_json
        
        llm_worst = llm_data.get("worst_case", "N/A")
        llm_best = llm_data.get("best_case", "N/A")
        llm_average = llm_data.get("average_case", "N/A")
        
        # Validar tanto ecuaciÃ³n como soluciÃ³n
        agrees_equation = llm_data.get("agrees_with_our_equation", True)
        agrees_solution = llm_data.get("agrees_with_our_solution", True)
        agrees = agrees_equation and agrees_solution
        
        confidence = llm_data.get("confidence", 0.5)
        differences = llm_data.get("differences", "")
        llm_explanation = llm_data.get("explanation", "")
        
        return ComparisonResult(
            our_worst=our_worst,
            our_best=our_best,
            our_average=our_average,
            llm_worst=llm_worst,
            llm_best=llm_best,
            llm_average=llm_average,
            agrees=agrees,
            confidence=confidence,
            differences=differences,
            llm_explanation=llm_explanation,
            latency_ms=response.latency_ms,
            llm_raw_response=response.raw_text
        )


# ============================================================================
# DEMO
# ============================================================================

def demo_iterative():
    """Demo: ComparaciÃ³n de algoritmo iterativo"""
    
    print("="*70)
    print("DEMO: COMPARACIÃ“N ITERATIVA CON LLM")
    print("="*70)
    
    code = """
BubbleSort(A[], n)
begin
    for i â† 1 to n-1 do
    begin
        for j â† 1 to n-i do
        begin
            if (A[j] > A[j+1]) then
            begin
                temp â† A[j]
                A[j] â† A[j+1]
                A[j+1] â† temp
            end
        end
    end
end
    """
    
    print("\nCÃ³digo:")
    print(code)
    
    # Nuestro anÃ¡lisis
    our_worst = "O(nÂ²)"
    our_best = "Î©(nÂ²)"
    our_average = "Î˜(nÂ²)"
    explanation = "FOR anidado: n iteraciones externas Ã— n-i internas â‰ˆ nÂ²"
    
    print(f"\nNuestro anÃ¡lisis: {our_worst}")
    
    # Comparar con LLM
    try:
        comparator = LLMComparator()
        
        print("\nğŸ“¤ Consultando LLM (Llama 3.3 70B)...")
        
        result = comparator.compare_iterative(
            code, our_worst, our_best, our_average, explanation
        )
        
        print(result)
        
        return result.agrees
    
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        return False


def demo_recursive():
    """Demo: ComparaciÃ³n de algoritmo recursivo"""
    
    print("\n" + "="*70)
    print("DEMO: COMPARACIÃ“N RECURSIVA CON LLM")
    print("="*70)
    
    code = """
MergeSort(A[], p, r)
begin
    if (p < r) then
    begin
        q â† floor((p + r) / 2)
        call MergeSort(A, p, q)
        call MergeSort(A, q+1, r)
        call Merge(A, p, q, r)
    end
end
    """
    
    print("\nCÃ³digo:")
    print(code)
    
    # Nuestro anÃ¡lisis
    our_equation = "T(n) = 2T(n/2) + n"
    our_worst = "O(n log n)"
    our_best = "Î©(n log n)"
    our_average = "Î˜(n log n)"
    explanation = "Teorema Maestro caso 2: a=2, b=2, f(n)=n â†’ Î˜(n log n)"
    
    print(f"\nNuestra ecuaciÃ³n: {our_equation}")
    print(f"Nuestra soluciÃ³n: {our_worst}")
    
    # Comparar con LLM
    try:
        comparator = LLMComparator()
        
        print("\nğŸ“¤ Consultando LLM (Llama 3.3 70B)...")
        
        result = comparator.compare_recursive(
            code, our_equation, our_worst, our_best, our_average, explanation
        )
        
        print(result)
        
        return result.agrees
    
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        return False


if __name__ == "__main__":
    print("="*70)
    print("COMPARADOR LLM - VALIDACIÃ“N DE ANÃLISIS")
    print("="*70)
    print()
    
    # Verificar API key
    api_key = os.getenv('GROQ_API_KEY')
    if not api_key:
        print("âŒ GROQ_API_KEY no configurada")
        print("\nğŸ’¡ Configura tu API key gratuita:")
        print("   1. Ve a: https://console.groq.com/keys")
        print("   2. Crea una cuenta (gratis)")
        print("   3. Genera una API key")
        print("   4. export GROQ_API_KEY='tu-api-key'")
        sys.exit(1)
    
    # Ejecutar demos
    success_iter = demo_iterative()
    success_rec = demo_recursive()
    
    print("\n" + "="*70)
    print("RESUMEN")
    print("="*70)
    print(f"Iterativo: {'âœ“ PASS' if success_iter else 'âœ— FAIL'}")
    print(f"Recursivo: {'âœ“ PASS' if success_rec else 'âœ— FAIL'}")
    print("="*70)
    
    sys.exit(0 if (success_iter and success_rec) else 1)